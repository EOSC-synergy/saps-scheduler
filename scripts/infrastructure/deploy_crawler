#!/bin/bash
#
# Deploy the crawler component

#FIXME add classpath and class names to constants

if [[ $# -ne 3 ]]; then
  echo "Usage:" $0 "path/to/private_key storage_size federation_member"
  exit 1
fi

PRIVATE_KEY_FILE=$1
STORAGE_SIZE=$2
FEDERATION_MEMBER=$3

if [[ ! -f "${PRIVATE_KEY_FILE}" ]]; then
  echo $PRIVATE_KEY_FILE "is not a regular file or does not exist"
  exit 1
fi

if [[ ! "${STORAGE_SIZE}" =~ ^[0-9]+$ ]]; then
  echo "Invalid storage_size" $STORAGE_SIZE
  exit 1
fi

#FIXME not sure if we should keep these declarations here or to load from a config file
#GLOBAL CONSTANTS
REMOTE_BASE_DIR=/tmp
SANDBOX_DIR=/local/esdras/git/sebal-engine
VOMS_CERT_FOLDER=/tmp
VOMS_CERT_FILE=x509up_u1210
LOCAL_REPOSITORY_PATH=/local/esdras
REMOTE_REPOSITORY_PATH=/home/fogbow
REMOTE_VOMS_CERT_FOLDER=/home/fogbow/Dev/keys/cert/
CONFIG_FILE=$SANDBOX_DIR/config/sebal.conf
SPEC_FILE=$SANDBOX_DIR/config/crawlerSpec
SCHEDULER_EXEC_INFO_FILE=$SANDBOX_DIR/scheduler/scheduler-info/scheduler-exec.info
FETCHER_EXEC_INFO_FILE=$SANDBOX_DIR/fetcher/fetcher-info/fetcher-exec.info
SEBAL_ENGINE_PKG_FILE=sebal-engine-pkg.tar.gz
MANAGER_PKG_FILE=manager-pkg.tar.gz

LOG4J=$SANDBOX_DIR/config/log4j.properties
if [[ -f ${LOG4J} ]]; then
  CONF_LOG=-Dlog4j.configuration=file:$LOG4J
else
  CONF_LOG=
fi

#Execution INFO
CRAWLER_EXECUTION_INFO=$SANDBOX_DIR/crawler/crawler-info/crawler-exec.info
if [ -f "$CRAWLER_EXECUTION_INFO" ]; then
        sudo rm $CRAWLER_EXECUTION_INFO
fi

if [ ! -e "$CRAWLER_EXECUTION_INFO" ]; then
	echo "Creating execution info file"
	touch $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_INSTANCE_ID=" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_USER_NAME=" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_INSTANCE_IP=" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_INSTANCE_PORT=" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_EXTRA_PORT=" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_STORAGE_ID=" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_STORAGE_ATTACHMENT_ID=" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_STORAGE_PORT=" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_STORAGE_FORMATED=NO" >> $CRAWLER_EXECUTION_INFO
	echo "CRAWLER_NFS_PORT=" >> $CRAWLER_EXECUTION_INFO
fi

source $CRAWLER_EXECUTION_INFO

#FIXME add some doc. i did not understand the purpose of below code
STORAGE_COMMAND="FORMAT";

if [ "$CRAWLER_STORAGE_FORMATED" = "YES" ]; then
	STORAGE_COMMAND="RETRIEVE";
fi

echo "Storage command: $STORAGE_COMMAND";

########### CRAWLER INFRASTRUCTURE ##############

# It starts crawler VM
# Globals
#   CONFIG_FILE
#   SPEC_FILE
# Returns
#   infrastructure description
function create_crawler_vm() {
  VM_CRAWLER_INFO=`java $CONF_LOG -cp $SANDBOX_DIR/target/sebal-scheduler-0.0.1-SNAPSHOT.jar:target/lib/* org.fogbowcloud.sebal.engine.infrastructure.InfrastructureMain compute $CONFIG_FILE $SPEC_FILE "false" 2>&1`

  if [[ ! $VM_CRAWLER_INFO == *"INSTANCE_ID"* ]]; then
    echo $VM_CRAWLER_INFO
    echo "There is no resource available for deploy Crawler App." >&2
    exit 1
  fi

  echo $VM_CRAWLER_INFO
}

#FIXME improve function. we need return values
# It Verifies if storage info exists.
# Globals
#   CRAWLER_STORAGE_ID
#   CONFIG_FILE
#   SPEC_FILE
# Args
#  CRAWLER_STORAGE_ID
function storage_exists() {
  if [ -n "$CRAWLER_STORAGE_ID" ]; then
    STORAGE_STATUS=`java $CONF_LOG -cp $SANDBOX_DIR/target/sebal-scheduler-0.0.1-SNAPSHOT.jar:target/lib/* org.fogbowcloud.sebal.engine.infrastructure.InfrastructureMain "test-storage" $DB_STORAGE_ID $CONFIG_FILE "false" 2>&1`
    STORAGE_STATUS=$(echo $STORAGE_STATUS | cut -d";" -f1 | cut -d"=" -f2)
    echo "Storage status: $STORAGE_STATUS";
    if [ ! $STORAGE_STATUS = "active" ]; then
      CRAWLER_STORAGE_ID="";
    fi
  fi
}

# It creates a volume
# Globals
# Args
# Returns
function create_volume() {
  #FIXME check args
  STORAGE_COMMAND="FORMAT";
  sed -i "/CRAWLER_STORAGE_FORMATED=/ s/=.*/=NO/" $CRAWLER_EXECUTION_INFO

  #This tow variables indicates if a new storage is been used. For new storage, the disk must be formated and the database must be created.
  STORAGE_INFO=`java $CONF_LOG -cp $SANDBOX_DIR/target/sebal-scheduler-0.0.1-SNAPSHOT.jar:target/lib/* org.fogbowcloud.sebal.engine.infrastructure.InfrastructureMain storage $STORAGE_SIZE $CONFIG_FILE $SPEC_FILE "false" 2>&1`
  echo $STORAGE_INFO

  CRAWLER_STORAGE_ID=$(echo $STORAGE_INFO | cut -d";" -f1 | cut -d"=" -f2)

  echo "New storage created: "$CRAWLER_STORAGE_ID
}

# it attaches the volume to the VM
# Globals
# Args
# Returns
function attach_volume() {
#FIXME check vars
#FIXME add proper return
  echo "Attaching $CRAWLER_STORAGE_ID to $INSTANCE_ID"
  CRAWLER_STORAGE_ATTACHMENT_INFO=`java $CONF_LOG -cp $SANDBOX_DIR/target/sebal-scheduler-0.0.1-SNAPSHOT.jar:target/lib/* org.fogbowcloud.sebal.engine.infrastructure.InfrastructureMain attachment $INSTANCE_ID $CRAWLER_STORAGE_ID $CONFIG_FILE $SPEC_FILE "false" 2>&1`

  if [[ ! $CRAWLER_STORAGE_ATTACHMENT_INFO == *"ATTACHMENT_ID"* ]]; then
    echo $CRAWLER_STORAGE_ATTACHMENT_INFO
    echo "Error while attaching $CRAWLER_STORAGE_ID to $INSTANCE_ID."
    exit
  fi

  echo $CRAWLER_STORAGE_ATTACHMENT_INFO;
  CRAWLER_STORAGE_ATTACHMENT_ID=$(echo $CRAWLER_STORAGE_ATTACHMENT_INFO | cut -d";" -f1 | cut -d"=" -f2)
  echo "Attach ID: $CRAWLER_STORAGE_ATTACHMENT_ID"
  #Update Storage INFO FILE with the new attachment id.
  sed -i "/CRAWLER_STORAGE_ATTACHMENT_ID=/ s/=.*/=$CRAWLER_STORAGE_ATTACHMENT_ID/" $CRAWLER_EXECUTION_INFO
}

#FIXME extract to a method
#Coping scripts to mount disk.
SCRIPTS_DIR="sebal_scripts"
LOCAL_FILE_PATH="$SANDBOX_DIR/scripts/infrastructure/sebal_scripts"
REMOTE_FILE_PATH="$REMOTE_BASE_DIR";

# SSH to crawler VM
# Globals
#   INSTANCE_PORT
#   PRIVATE_KEY_FILE
#   USER_NAME
#   INSTANCE_IP
# Args
#   remote_command
# Returns
function ssh_to_crawler() {
  #FIXME check remote_command?
  local remote_command=$*
  ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -p $INSTANCE_PORT -i $PRIVATE_KEY_FILE  $USER_NAME@$INSTANCE_IP ${remote_command}
}

# Globals
#   INSTANCE_PORT
#   PRIVATE_KEY_FILE
#   USER_NAME
#   INSTANCE_IP
# Args
#   src_path
#   dst_path
# Returns
function scp_to_crawler() {
  #FIXME how about the -r modified?
  local src_path=$1
  local dst_path=$2
  scp -r -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -P $INSTANCE_PORT -i $PRIVATE_KEY_FILE $src_path $USER_NAME@$INSTANCE_IP:/$dst_path
}

# Globals
#   EXTRA_PORT
# Args
# Returns
#  nfs_port
function parse_nfs_port() {
  IFS=',' read -r -a extraPortsArray <<< $EXTRA_PORT
  arrayLength=${#extraPortsArray[@]};
  for (( i=0; i<${arrayLength}; i++ ));
  do
    actualPort=${extraPortsArray[$i]};
    if [[ $actualPort == *"nfs"* ]]; then
      nfs_port=`echo $actualPort | cut -d":" -f3 | tr -d "}"`
      echo $nfs_port
    fi
  done
}

function execute_crawler_app() {
  DB_INSTANCE_IP=$(sed -n 3p $SCHEDULER_EXEC_INFO_FILE | cut -d"=" -f2)
  DB_PORT=$(sed -n 8p $SCHEDULER_EXEC_INFO_FILE | cut -d"=" -f2)

  crawlerapp_cmd="sudo sh $SANDBOX_DIR/scripts/cli/crawler $DB_INSTANCE_IP $DB_PORT $FEDERATION_MEMBER"
  ssh_to_crawler ${crawlerapp_cmd}
}

function main() {
  echo "Creating Crawler VM"
  VM_CRAWLER_INFO=`create_crawler_vm`
  
  #PREPARING VARIABLES FOR SSH/SCP
  #Sample return USER_NAME=fogbow;SSH_HOST=192.168.0.1;SSH_HOST=9000;SSH_HOST=
  INSTANCE_ID=$(echo $VM_CRAWLER_INFO | cut -d";" -f1 | cut -d"=" -f2)
  USER_NAME=$(echo $VM_CRAWLER_INFO | cut -d";" -f2 | cut -d"=" -f2)
  INSTANCE_IP=$(echo $VM_CRAWLER_INFO | cut -d";" -f3 | cut -d"=" -f2)
  INSTANCE_PORT=$(echo $VM_CRAWLER_INFO | cut -d";" -f4 | cut -d"=" -f2)
  EXTRA_PORT=$(echo $VM_CRAWLER_INFO | cut -d";" -f5 | cut -d"=" -f2)
  FETCHER_PUBLIC_KEY=$(cat $FETCHER_EXEC_INFO_FILE | cut -d";" -f6 | cut -d"=" -f2)

  echo "Checking if storage exists"
  storage_exists

  echo "Creating volume"
  create_volume

  echo "Attaching volume"
  attach_volume

  echo "Uploading scripts"
  scp_to_crawler $LOCAL_FILE_PATH $REMOTE_FILE_PATH

  echo "Change script dir permission"
  chmod_cmd="sudo chmod -R 777 $REMOTE_FILE_PATH/$SCRIPTS_DIR"
  ssh_to_crawler ${chmod_cmd}

  echo "Changing VM timezone"
  timezone_cmd="sudo sh $REMOTE_FILE_PATH/$SCRIPTS_DIR/change_timezone.sh"
  ssh_to_crawler ${timezone_cmd}

  echo "Mounting volume"
  mount_cmd="sudo sh $REMOTE_FILE_PATH/$SCRIPTS_DIR/mount_partition.sh $STORAGE_COMMAND"
  ssh_to_crawler ${mount_cmd}
  sed -i "/CRAWLER_STORAGE_FORMATED=/ s/=.*/=YES/" $CRAWLER_EXECUTION_INFO

  echo "Putting Fetcher public key into authorized keys"
  auth_key_cmd="sudo sh $REMOTE_FILE_PATH/$SCRIPTS_DIR/authorize_fetcher_key.sh $FETCHER_PUBLIC_KEY"
  ssh_to_crawler ${auth_key_cmd}

  echo "Installing USGS API in Crawler VM"
  usgs_install_cmd="sudo sh $REMOTE_FILE_PATH/$SCRIPTS_DIR/install_usgs_api.sh"
  ssh_to_crawler ${usgs_install_cmd}

  # FIXME remember to put here a ftp install sh

  echo "Upload crawler packages"
  sudo sh $SANDBOX_DIR/scripts/infrastructure/$SCRIPTS_DIR/pack_sebal_engine_repository.sh $LOCAL_REPOSITORY_PATH $SANDBOX_DIR
  sudo sh $SANDBOX_DIR/scripts/infrastructure/$SCRIPTS_DIR/pack_manager_repository.sh $LOCAL_REPOSITORY_PATH
  scp_to_crawler $LOCAL_REPOSITORY_PATH/$SEBAL_ENGINE_PKG_FILE $REMOTE_REPOSITORY_PATH
  scp_to_crawler $LOCAL_REPOSITORY_PATH/$MANAGER_PKG_FILE $REMOTE_REPOSITORY_PATH
  
  echo "Upload certificates"
  LOCAL_FILE_PATH="$VOMS_CERT_FOLDER/$VOMS_CERT_FILE"
  FILE_PATH="$REMOTE_VOMS_CERT_FOLDER/$VOMS_CERT_FILE"
  mkdir_cmd="sudo mkdir -p $FILE_PATH"
  chmod_cmd="sudo chmod 777 $FILE_PATH"
  ssh_to_crawler ${mkdir_cmd}
  ssh_to_crawler ${chmod_cmd}
  scp_to_crawler $LOCAL_FILE_PATH $FILE_PATH

  CRAWLER_NFS_PORT=`parse_nfs_port`

  echo "Preparing log4j dir"
  mkdir_log4j_cmd="sudo mkdir -p /var/log/sebal-execution"
  touch_log4j_cmd="sudo touch /var/log/sebal-execution/sebal-execution.log"
  chmod_log4j_cmd="sudo chmod 777 /var/log/sebal-execution/sebal-execution.log"
  ssh_to_crawler ${mkdir_log4j_cmd}
  ssh_to_crawler ${touch_log4j_cmd}
  ssh_to_crawler ${chmod_log4j_cmd}

  #Putting informations on Crawler execution info.
  sed -i "/CRAWLER_INSTANCE_ID=/ s/=.*/=$INSTANCE_ID/" $CRAWLER_EXECUTION_INFO
  sed -i "/CRAWLER_USER_NAME=/ s/=.*/=$USER_NAME/" $CRAWLER_EXECUTION_INFO
  sed -i "/CRAWLER_INSTANCE_IP=/ s/=.*/=$INSTANCE_IP/" $CRAWLER_EXECUTION_INFO
  sed -i "/CRAWLER_INSTANCE_PORT=/ s/=.*/=$INSTANCE_PORT/" $CRAWLER_EXECUTION_INFO
  sed -i "/CRAWLER_EXTRA_PORT=/ s/=.*/=$EXTRA_PORT/" $CRAWLER_EXECUTION_INFO
  #Update STORAGE INFO FILE with the new attachment id.
  sed -i "/CRAWLER_STORAGE_ATTACHMENT_ID=/ s/=.*/=$CRAWLER_STORAGE_ATTACHMENT_ID/" $CRAWLER_EXECUTION_INFO
  sed -i "/CRAWLER_STORAGE_ID=/ s/=.*/=$CRAWLER_STORAGE_ID/" $CRAWLER_EXECUTION_INFO
  sed -i "/CRAWLER_NFS_PORT=/ s/=.*/=$CRAWLER_NFS_PORT/" $CRAWLER_EXECUTION_INFO

  #Remember to add federation_member param here
  #echo "Executing crawler app"
  #execute_crawler_app
}

#do deploy
main
